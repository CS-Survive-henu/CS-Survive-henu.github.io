# åˆ†å¸ƒå¼è®­ç»ƒå®æˆ˜æŒ‡å— (Distributed Training Practical Guide)

## æ¦‚è¿° (Overview)

åˆ†å¸ƒå¼è®­ç»ƒæ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§è§„æ¨¡æ¨¡å‹å’Œæ•°æ®é›†æ—¶ã€‚æœ¬æŒ‡å—æä¾›äº†å®Œæ•´çš„åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)è®­ç»ƒè§£å†³æ–¹æ¡ˆï¼Œè§£å†³äº†å¸¸è§çš„è®­ç»ƒé—®é¢˜ã€‚

Distributed training is a crucial technique in deep learning, especially when dealing with large-scale models and datasets. This guide provides a complete Distributed Data Parallel (DDP) training solution that addresses common training issues.

## ğŸ¯ è§£å†³çš„æ ¸å¿ƒé—®é¢˜ (Core Problems Solved)

### 1. DDPåŒæ­¥é”™è¯¯ (DDP Synchronization Errors)
**é—®é¢˜**: `RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one`

**åŸå› åˆ†æ**:
- ä¸åŒè¿›ç¨‹ä¹‹é—´çš„å‰å‘ä¼ æ’­ä¸ä¸€è‡´
- æŸäº›å‚æ•°åœ¨æŸäº›rankä¸Šæœªè¢«ä½¿ç”¨
- æ¢¯åº¦ç´¯ç§¯å¤„ç†ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿æ‰€æœ‰rankæ‰§è¡Œç›¸åŒçš„å‰å‘ä¼ æ’­
# Ensure consistent forward pass across all ranks
model = DDP(model, find_unused_parameters=False)  # ç”Ÿäº§ç¯å¢ƒå»ºè®®False

# å‘¨æœŸæ€§åŒæ­¥æ£€æŸ¥ç‚¹
if step % 100 == 0:
    torch.distributed.barrier()
```

### 2. DataLoaderè¿›ç¨‹å´©æºƒ (DataLoader Worker Crashes)
**é—®é¢˜**: `DataLoader worker (pid XXXX) is killed by signal: Aborted`

**åŸå› åˆ†æ**:
- å…±äº«å†…å­˜ä¸è¶³
- å¤šè¿›ç¨‹é…ç½®é”™è¯¯
- å†…å­˜æ³„æ¼

**è§£å†³æ–¹æ¡ˆ**:
```python
# è‡ªåŠ¨è®¡ç®—æœ€ä¼˜workeræ•°é‡
cpu_count = psutil.cpu_count(logical=False)
num_workers = min(cpu_count, 8) // world_size

# ä½¿ç”¨æ›´å®‰å…¨çš„å¤šè¿›ç¨‹ä¸Šä¸‹æ–‡
dataloader = DataLoader(
    dataset,
    num_workers=num_workers,
    multiprocessing_context=mp.get_context('spawn'),
    persistent_workers=True,
    timeout=30
)
```

### 3. å‚æ•°æ¢¯åº¦ç¼ºå¤± (Missing Parameter Gradients)
**é—®é¢˜**: æŸäº›rankä¸Šçš„å‚æ•°æœªæ¥æ”¶åˆ°æ¢¯åº¦

**è§£å†³æ–¹æ¡ˆ**:
```python
# å‚æ•°ä½¿ç”¨è·Ÿè¸ª
class ModelWithTracking(nn.Module):
    def forward(self, x):
        self._used_parameters.add('layer_name')
        return self.layer(x)
    
    def get_unused_parameters(self):
        # è¿”å›æœªä½¿ç”¨çš„å‚æ•°åˆ—è¡¨
        pass
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ (Quick Start)

### ç¯å¢ƒå‡†å¤‡ (Environment Setup)
```bash
# å®‰è£…ä¾èµ–
pip install torch torchvision numpy tqdm psutil

# è®¾ç½®è°ƒè¯•ç¯å¢ƒå˜é‡
export TORCH_DISTRIBUTED_DEBUG=DETAIL
export NCCL_DEBUG=INFO
```

### åŸºç¡€è®­ç»ƒå‘½ä»¤ (Basic Training Commands)

1. **å•GPUè®­ç»ƒ** (Single GPU Training)
   ```bash
   python ddp_training.py --single_gpu --batch_size 32 --epochs 5
   ```

2. **å¤šGPUè®­ç»ƒ** (Multi-GPU Training)
   ```bash
   # 2ä¸ªGPU
   torchrun --nproc_per_node=2 ddp_training.py --batch_size 16 --epochs 5
   
   # 4ä¸ªGPU
   torchrun --nproc_per_node=4 ddp_training.py --batch_size 8 --epochs 5
   ```

3. **è°ƒè¯•æ¨¡å¼** (Debug Mode)
   ```bash
   torchrun --nproc_per_node=2 ddp_training.py --debug --log_interval 10
   ```

## ğŸ”§ é«˜çº§é…ç½® (Advanced Configuration)

### å†…å­˜ä¼˜åŒ– (Memory Optimization)
```bash
# ä¿å®ˆæ¨¡å¼ï¼šå‡å°‘å†…å­˜ä½¿ç”¨
torchrun --nproc_per_node=2 ddp_training.py \
    --batch_size 8 \
    --num_workers 0 \
    --find_unused_parameters \
    --gradient_clip_val 1.0
```

### é”™è¯¯æ¢å¤ (Error Recovery)
```bash
# å¯ç”¨é”™è¯¯æ¢å¤æœºåˆ¶
torchrun --nproc_per_node=2 ddp_training.py \
    --continue_on_error \
    --skip_invalid_loss \
    --batch_size 16
```

### æ€§èƒ½ä¼˜åŒ– (Performance Optimization)
```bash
# æ€§èƒ½ä¼˜åŒ–é…ç½®
export NCCL_TREE_THRESHOLD=0
export NCCL_MIN_NRINGS=4

torchrun --nproc_per_node=4 ddp_training.py \
    --batch_size 32 \
    --num_workers 4 \
    --gradient_clip_val 1.0
```

## ğŸ“Š ç›‘æ§å’Œè°ƒè¯• (Monitoring and Debugging)

### å†…å­˜ç›‘æ§ (Memory Monitoring)
```python
def log_memory_usage():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**2
        cached = torch.cuda.memory_reserved() / 1024**2
        print(f"GPUå†…å­˜ - å·²åˆ†é…: {allocated:.2f}MB, ç¼“å­˜: {cached:.2f}MB")
```

### æ€§èƒ½åˆ†æ (Performance Profiling)
```python
# ä½¿ç”¨PyTorch profiler
with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA]
) as prof:
    model(batch)

print(prof.key_averages().table(sort_by="cuda_time_total"))
```

## ğŸ› ï¸ æ•…éšœæ’é™¤ (Troubleshooting)

### å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ (Common Errors and Solutions)

1. **CUDAå†…å­˜ä¸è¶³** (CUDA Out of Memory)
   ```python
   # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
   from torch.utils.checkpoint import checkpoint
   output = checkpoint(model_layer, input)
   
   # æ··åˆç²¾åº¦è®­ç»ƒ
   from torch.cuda.amp import autocast, GradScaler
   scaler = GradScaler()
   with autocast():
       loss = model(batch)
   ```

2. **è¿›ç¨‹æŒ‚èµ·** (Process Hanging)
   ```bash
   # è®¾ç½®è¶…æ—¶
   export NCCL_BLOCKING_WAIT=1
   export NCCL_ASYNC_ERROR_HANDLING=1
   
   # ä½¿ç”¨è¶…æ—¶åˆå§‹åŒ–
   torch.distributed.init_process_group(
       backend='nccl',
       timeout=datetime.timedelta(seconds=300)
   )
   ```

3. **ç½‘ç»œé…ç½®é—®é¢˜** (Network Configuration Issues)
   ```bash
   # InfiniBandç½‘ç»œ
   export NCCL_IB_DISABLE=1
   export NCCL_SOCKET_IFNAME=eth0
   
   # è°ƒè¯•ç½‘ç»œé—®é¢˜
   export NCCL_DEBUG_SUBSYS=NET
   ```

## ğŸ“š å­¦ä¹ èµ„æº (Learning Resources)

### æ¨èé˜…è¯» (Recommended Reading)
1. [PyTorchå®˜æ–¹DDPæ•™ç¨‹](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
2. [NCCLæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html)
3. [åˆ†å¸ƒå¼è®­ç»ƒæœ€ä½³å®è·µ](https://pytorch.org/tutorials/recipes/recipes/distributed_training.html)

### å®è·µé¡¹ç›®å»ºè®® (Practical Project Suggestions)
1. **å…¥é—¨é¡¹ç›®**: åœ¨CIFAR-10ä¸Šå®ç°ç®€å•çš„åˆ†å¸ƒå¼å›¾åƒåˆ†ç±»
2. **è¿›é˜¶é¡¹ç›®**: ä½¿ç”¨å¤šèŠ‚ç‚¹è®­ç»ƒå¤§å‹è¯­è¨€æ¨¡å‹
3. **ä¸“å®¶é¡¹ç›®**: å®ç°è‡ªå®šä¹‰åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥

## ğŸ“ æ•™å­¦è¦ç‚¹ (Teaching Points)

### æ ¸å¿ƒæ¦‚å¿µ (Core Concepts)
1. **æ•°æ®å¹¶è¡Œ vs æ¨¡å‹å¹¶è¡Œ** (Data Parallel vs Model Parallel)
2. **æ¢¯åº¦åŒæ­¥æœºåˆ¶** (Gradient Synchronization)
3. **å†…å­˜ç®¡ç†ç­–ç•¥** (Memory Management Strategies)

### å®è·µæŠ€èƒ½ (Practical Skills)
1. **ç¯å¢ƒé…ç½®å’Œè°ƒè¯•** (Environment Setup and Debugging)
2. **æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–** (Performance Monitoring and Optimization)
3. **é”™è¯¯è¯Šæ–­å’Œå¤„ç†** (Error Diagnosis and Handling)

## ğŸ’¡ ç”Ÿäº§ç¯å¢ƒå»ºè®® (Production Recommendations)

1. **ç›‘æ§ç³»ç»Ÿ**: é›†æˆPrometheus + Grafanaè¿›è¡Œç›‘æ§
2. **æ—¥å¿—ç®¡ç†**: ä½¿ç”¨ELK stackæ”¶é›†å’Œåˆ†ææ—¥å¿—
3. **è‡ªåŠ¨åŒ–éƒ¨ç½²**: ä½¿ç”¨Kubernetesè¿›è¡Œå®¹å™¨åŒ–éƒ¨ç½²
4. **é”™è¯¯æ¢å¤**: å®ç°æ£€æŸ¥ç‚¹å’Œè‡ªåŠ¨é‡å¯æœºåˆ¶

## ğŸ“ˆ æœªæ¥å‘å±• (Future Developments)

1. **æ–°æŠ€æœ¯**: å…³æ³¨ZeROã€FairScaleç­‰æ–°çš„åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯
2. **ç¡¬ä»¶ä¼˜åŒ–**: é’ˆå¯¹æ–°GPUæ¶æ„(å¦‚H100)çš„ä¼˜åŒ–
3. **äº‘åŸç”Ÿ**: åœ¨äº‘ç¯å¢ƒä¸­çš„æœ€ä½³å®è·µ

---

*è¿™ä¸ªæŒ‡å—å°†æŒç»­æ›´æ–°ï¼Œä»¥åæ˜ æœ€æ–°çš„æœ€ä½³å®è·µå’ŒæŠ€æœ¯å‘å±•ã€‚*

*This guide will be continuously updated to reflect the latest best practices and technological developments.*